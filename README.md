This directory contains Images used for training and validation at the Zuzan city Neural networks research paper, and the Code for the Yolo (8) implementation of the CNN

READ.ME File: Relationship between code and figures and tables in the manuscript

The YOLO model is trained using the script yolo/train.py, which processes the training data provided in the data/ directory.
Model evaluation on the test dataset and the computation of quantitative performance metrics are performed using yolo/test.py.
The quantitative results reported in the manuscript and presented in the tables are obtained from the outputs of yolo/test.py.

Figure 4 presents qualitative detection results generated during the training process by yolo/train.py. Other experimental figures are produced based on the outputs saved in the results/ directory.

The script utils.py contains auxiliary functions shared by the training and evaluation scripts and does not directly generate figures or tables.

Table 1 in the paper. Generated by: scripts/evaluate_model.py
Backbone
Input (800×800×3)
↓
Conv + Downsample
↓
C2f × 23 blocks
↓
Total convolutional layers: 93

Neck (YOLOv8 PAN-FPN)
PAN-FPN:
- PANet layers: 5
- Up-sampling layers: 3
- Down-sampling layers: 3
  
Head
Single detection head
(anchor-free, decoupled head)

The proposed model is based on the YOLOv8 architecture.
The backbone consists of 93 convolutional layers and 23 C2f blocks, enabling efficient feature extraction while maintaining a lightweight design.
The neck adopts a PAN-FPN structure composed of 5 PANet layers, 3 up-sampling layers, and 3 down-sampling layers to enhance multi-scale feature fusion.
Finally, a single anchor-free detection head is employed to generate the final predictions. 

Figure 6: 
1-import
from ultralytics import YOLO
import numpy as np
import pandas as pd
from sklearn.model_selection import KFold
from sklearn.metrics import roc_auc_score

2-Cross-Validation
K = 5  # 5-fold cross-validation
kf = KFold(n_splits=K, shuffle=True, random_state=42)
image_paths = np.array(all_image_paths)    
labels = np.array(all_labels)               # 1: qanat, 0: no-qanat

 3-Performing Cross-Validation and Probability Extraction
all_fold_results = []
for fold, (train_idx, test_idx) in enumerate(kf.split(image_paths)):
    print(f"Fold {fold+1}/{K}")
    train_images = image_paths[train_idx]
    test_images  = image_paths[test_idx]
    test_labels  = labels[test_idx]

# Load trained YOLOv8 model
    model = YOLO("trained_yolov8_model.pt")
    probabilities = []
    for img_path in test_images:
        results = model(img_path, conf=0.0, iou=0.5)
        if len(results[0].boxes) > 0:
            # Highest confidence detection
            prob = results[0].boxes.conf.max().item()
        else:
            prob = 0.0  # no detection → no qanat
        probabilities.append(prob)

    fold_df = pd.DataFrame({
        "Fold": fold + 1,
        "Image": test_images,
        "True_Label": test_labels,
        "Predicted_Probability": probabilities
    })

    all_fold_results.append(fold_df)

4-  Aggregation of Cross-Validation Results
results_df = pd.concat(all_fold_results, ignore_index=True)
results_df.to_csv("qanat_probability_cross_validation.csv", index=False)

5- Calculating the validation criteria

auc = roc_auc_score(
    results_df["True_Label"],
    results_df["Predicted_Probability"]
)

print("Cross-validated AUC:", auc)

Cross-validation was performed to assess the robustness of the proposed model.
For each fold, the trained YOLOv8 model outputs probability values representing the confidence of qanat presence. These probabilities were validated against
the corresponding labeled test data to evaluate model performance.

Figure8: 
  !yolo task=detect \
  mode=predict \
  model=best.pt \
  conf=0.1 \
  source=zuzan2.jpg
 
Figure9: 
  !yolo task=detect \
  mode=predict \
  model=best.pt \
  conf=0.1 \
  source=zuzan3.jpg
For more accurate recognition, the image was divided into 16 parts. Then, it was predicted with the above command.
